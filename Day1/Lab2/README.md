## LAB 2 â€” Pipeline batch distribuÃ© (logs web)

### ğŸ“ lab2_pipeline_batch/README.md

### ğŸ¯ Objectifs pÃ©dagogiques

* CrÃ©er un pipeline de traitement distribuÃ© sur plusieurs fichiers.
* Explorer le partitionnement et le parallÃ©lisme.
* GÃ©nÃ©rer des agrÃ©gations temporelles.

---

### ğŸ“š Concepts abordÃ©s

1. Lecture de donnÃ©es volumineuses (CSV, JSON, HDFS)
2. Pipeline ETL batch
3. OpÃ©rations groupÃ©es et jointures
4. Sauvegarde en Parquet partitionnÃ©

---

### ğŸ§© Ã‰tapes
-1ï¸âƒ£ Charger les logs web
```python
df_logs = spark.read.option("header", True).csv("../data/logs_web.csv")
```

- 2ï¸âƒ£ Extraire les champs utiles
```python
from pyspark.sql.functions import split, col
df_logs = df_logs.withColumn("url_path", split(col("url"), "/").getItem(1))
```

- 3ï¸âƒ£ AgrÃ©gation par heure
```python
from pyspark.sql.functions import hour, count
df_stats = df_logs.groupBy(hour(col("timestamp")).alias("hour")).agg(count("*").alias("visits"))
```

- 4ï¸âƒ£ Sauvegarder
```python
df_stats.write.mode("overwrite").parquet("../data/output/logs_hourly/")
```

---

### ğŸ§  Ã€ faire : Assiignements

- Ajouter une agrÃ©gation par navigateur (user_agent).
- Sauvegarder un top 10 des pages les plus vues.
- Documenter les performances (df.count(), df.rdd.getNumPartitions()).

---

### âœ… Livrable

* Script : pipeline_logs.py
* Dossier : output/logs_hourly/
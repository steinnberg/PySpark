# ğŸ” LAB 4 â€” Streaming en temps rÃ©el (capteurs IoT)

## ğŸ“ lab4_streaming_realtime/README.md

### ğŸ¯ Objectifs pÃ©dagogiques

- Mettre en place un pipeline Spark Streaming.
- Lire des flux temps rÃ©el (socket/Kafka).
- Appliquer des agrÃ©gations sur fenÃªtres temporelles.

---

### ğŸ“š Concepts abordÃ©s

- Spark Structured Streaming
- Source Socket/Kafka
- FenÃªtrage temporel
- Sink (console, fichier, base)

---

### ğŸ§© Ã‰tapes

- 1ï¸âƒ£ CrÃ©er une session Spark Streaming
```python
spark = SparkSession.builder.appName("Streaming-IoT").getOrCreate()
```
- 2ï¸âƒ£ Lecture depuis une socket (simulation)

Dans un terminal :
```bash
nc -lk 9999
```

Puis envoie quelques lignes JSON :
```json
{"sensor_id": 1, "temp": 23.4, "timestamp": "2025-10-11T09:00:00"}
```

- 3ï¸âƒ£ Lecture du flux :

```python
df_stream = spark.readStream.format("socket").option("host", "localhost").option("port", 9999).load()
```

- 4ï¸âƒ£ Parsing JSON et agrÃ©gation
```python
from pyspark.sql.functions import from_json, col, avg, window
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

schema = StructType([
    StructField("sensor_id", StringType()),
    StructField("temp", DoubleType()),
    StructField("timestamp", StringType())
])

df_parsed = df_stream.select(from_json(col("value"), schema).alias("data")).select("data.*")

df_agg = df_parsed.groupBy(
    window(col("timestamp"), "5 minutes"), col("sensor_id")
).agg(avg("temp").alias("avg_temp"))

```
- 5ï¸âƒ£ Sortie vers console
```python
query = df_agg.writeStream.outputMode("complete").format("console").start()
query.awaitTermination()
```
---

### ğŸ§  Ã€ faire par lâ€™apprenant

- Ajouter une fenÃªtre de 1 min glissante.
- Sauvegarder les rÃ©sultats en Parquet (sink = â€œparquetâ€).
- Bonus : connecter une source Kafka.

---

### âœ… Livrable

1. Script : streaming_iot.py
2. Fichier : output/streaming_temp.parquet
3. Capture : exemple du flux en console.